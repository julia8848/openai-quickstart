{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c89196-4259-4e3d-aa0d-30c7371ec403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3d130-dc10-445c-8499-ef7d42b93506",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16974ce-b493-4fc5-afc8-adf4d643e3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1faff-4efb-4e73-9a2b-ae1ec60fc030",
   "metadata": {},
   "source": [
    "### Homework 1: 使用其他的线上文档或离线文件，重新构建向量数据库\n",
    "线上文档使用Gaze Follower的论文：https://people.csail.mit.edu/khosla/papers/nips2015_recasens.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e2897c8-b923-4eaa-a920-d421f2644a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\n",
    "    file_path = \"https://people.csail.mit.edu/khosla/papers/nips2015_recasens.pdf\"\n",
    ")\n",
    "docs = await loader.aload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c7d6063-36c3-405d-8804-9691cb50c5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2640\n"
     ]
    }
   ],
   "source": [
    "print(len(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69ca0e12-ca69-48ed-ad5e-b809f08777fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where are they looking?\n",
      "Adri`a Recasens∗ Aditya Khosla∗ Carl Vondrick Antonio Torralba\n",
      "Massachusetts\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78a7fc-5a07-4c94-af40-e48e21a1aa03",
   "metadata": {},
   "source": [
    "## Split Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ce81ba-d8dd-460f-944f-cafaf7ff79d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 RecursiveCharacterTextSplitter 将文档分割成块，每块1000字符，重叠200字符\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cd25237-d16f-45e7-99f8-c0933d2d3787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(all_splits))  # 打印分割后的文档块数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d83e0538-1b88-45ec-95e8-05f7508e441b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947\n"
     ]
    }
   ],
   "source": [
    "print(len(all_splits[0].page_content))  # 打印第一个块的字符数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec937d9e-8a7f-4bf1-be7d-8c4f725ed816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where are they looking?\n",
      "Adri`a Recasens∗ Aditya Khosla∗ Carl Vondrick Antonio Torralba\n",
      "Massachusetts Institute of Technology\n",
      "{recasens, khosla, vondrick, torralba}@csail.mit.edu\n",
      "(* - indicates equal contribution)\n",
      "Abstract\n",
      "Humans have the remarkable ability to follow the gaze of other people to identify\n",
      "what they are looking at. Following eye gaze, or gaze-following, is an important\n",
      "ability that allows us to understand what other people are thinking, the actions they\n",
      "are performing, and even predict what they might do next. Despite the importance\n",
      "of this topic, this problem has only been studied in limited scenarios within the\n",
      "computer vision community. In this paper, we propose a deep neural network-\n",
      "based approach for gaze-following and a new benchmark dataset, GazeFollow, for\n",
      "thorough evaluation. Given an image and the location of a head, our approach\n",
      "follows the gaze of the person and identiﬁes the object being looked at. Our deep\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[0].page_content)  # 打印第一个块的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87978a76-e28e-483e-a284-18a698f5fcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://people.csail.mit.edu/khosla/papers/nips2015_recasens.pdf', 'page': 0, 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[0].metadata)  # 打印第一个块的元数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b04cf3-4041-4033-890b-cec31e05d11a",
   "metadata": {},
   "source": [
    "## Embedding and Store in vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfb0fcb5-d0f8-4a7a-af27-bc30556aa2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Chroma 向量存储和 OpenAIEmbeddings 模型，将分割的文档块嵌入并存储\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dede196b-1c26-4c95-ba4a-1632577a4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 VectorStoreRetriever 从向量存储中检索与查询最相关的文档\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9565b57f-9f13-45b6-9341-859b8c3a242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is the main purpose of this article?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f998474e-bf8f-4305-836b-204f749af490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# 检查检索到的文档内容\n",
    "print(len(retrieved_docs))  # 打印检索到的文档数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca08aa3b-fadf-4671-a4b0-71235cc28895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://gazefollow.csail.mit.edu.\n",
      "1 Introduction\n",
      "You step out of your house and notice a group of people looking up. You look up and realize they are\n",
      "looking at an aeroplane in the sky. Despite the object being far away, humans have the remarkable\n",
      "ability to precisely follow the gaze direction of another person, a task commonly referred to asgaze-\n",
      "following (see [3] for a review). Such an ability is a key element to understanding what people are\n",
      "doing in a scene and their intentions. Similarly, it is crucial for a computer vision system to have\n",
      "this ability to better understand and interpret people. For instance, a person might be holding a book\n",
      "but looking at the television, or a group of people might be looking at the same object which can\n",
      "indicate that they are collaborating at some task, or they might be looking at different places which\n",
      "Figure 1: Gaze-following: We present a model that learns to predict where people in images are\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)  # 打印第一个检索到的文档内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7166dc-3ea8-446a-ae5b-f347b19e6470",
   "metadata": {},
   "source": [
    "## Set LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47074d7f-9335-4918-81ce-73e1ad101091",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e51d2-e9d8-48c4-b296-ca4a017df9ed",
   "metadata": {},
   "source": [
    "## Create Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bdb6a7-04a1-4a98-85f6-1d5856b6636f",
   "metadata": {},
   "source": [
    "### Homework 2: 重新设计或在 LangChain Hub 上找一个可用的 RAG 提示词模板，测试对比两者的召回率和生成质量。\n",
    "提示词模板使用：https://smith.langchain.com/hub/krunal/more-crafted-rag-prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a27ce5c-dd8d-46c5-a040-ee8a5ca45f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zy/miniconda3/envs/llm-dev/lib/python3.12/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"krunal/more-crafted-rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3e5cc44-d49c-4763-bab9-3648d054cbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Your role\n",
      "You are a brilliant expert at understanding the intent of the questioner and the crux of the question, and providing the most optimal answer to the questioner's needs from the documents you are given.\n",
      "\n",
      "\n",
      "# Instruction\n",
      "Your task is to answer the question using the following pieces of retrieved context delimited by XML tags.\n",
      "\n",
      "<retrieved context>\n",
      "Retrieved Context:\n",
      "{context}\n",
      "</retrieved context>\n",
      "\n",
      "\n",
      "# Constraint\n",
      "1. Think deeply and multiple times about the user's question\\nUser's question:\\n{question}\\nYou must understand the intent of their question and provide the most appropriate answer.\n",
      "- Ask yourself why to understand the context of the question and why the questioner asked it, reflect on it, and provide an appropriate response based on what you understand.\n",
      "2. Choose the most relevant content(the key content that directly relates to the question) from the retrieved context and use it to generate an answer.\n",
      "3. Generate a concise, logical answer. When generating the answer, Do Not just list your selections, But rearrange them in context so that they become paragraphs with a natural flow. \n",
      "4. When you don't have retrieved context for the question or If you have a retrieved documents, but their content is irrelevant to the question, you should answer 'I can't find the answer to that question in the material I have'.\n",
      "5. Use five sentences maximum. Keep the answer concise but logical/natural/in-depth.\n",
      "\n",
      "\n",
      "# Question:\n",
      "{question}\n"
     ]
    }
   ],
   "source": [
    "# 打印模板\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40bcaf76-3dfc-4606-b297-8a5950092c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为 context 和 question 填充样例数据，并生成 ChatModel 可用的 Messages\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dce4827-7b29-4703-8d5e-a0d95bfb18e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Your role\n",
      "You are a brilliant expert at understanding the intent of the questioner and the crux of the question, and providing the most optimal answer to the questioner's needs from the documents you are given.\n",
      "\n",
      "\n",
      "# Instruction\n",
      "Your task is to answer the question using the following pieces of retrieved context delimited by XML tags.\n",
      "\n",
      "<retrieved context>\n",
      "Retrieved Context:\n",
      "filler context\n",
      "</retrieved context>\n",
      "\n",
      "\n",
      "# Constraint\n",
      "1. Think deeply and multiple times about the user's question\\nUser's question:\\nfiller question\\nYou must understand the intent of their question and provide the most appropriate answer.\n",
      "- Ask yourself why to understand the context of the question and why the questioner asked it, reflect on it, and provide an appropriate response based on what you understand.\n",
      "2. Choose the most relevant content(the key content that directly relates to the question) from the retrieved context and use it to generate an answer.\n",
      "3. Generate a concise, logical answer. When generating the answer, Do Not just list your selections, But rearrange them in context so that they become paragraphs with a natural flow. \n",
      "4. When you don't have retrieved context for the question or If you have a retrieved documents, but their content is irrelevant to the question, you should answer 'I can't find the answer to that question in the material I have'.\n",
      "5. Use five sentences maximum. Keep the answer concise but logical/natural/in-depth.\n",
      "\n",
      "\n",
      "# Question:\n",
      "filler question\n"
     ]
    }
   ],
   "source": [
    "# 查看提示词\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c795e-17ea-45b3-8b7c-c29edf66949f",
   "metadata": {},
   "source": [
    "## Use LCEL to build RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1127dda-f62b-486c-a7f9-4769a7fa5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义格式化文档的函数\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce9710c2-a65d-4fd1-a255-35f82470e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 LCEL 构建 RAG Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783472c4-31e0-4f37-97a4-974b9e4ab384",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d25db2-0b7f-4edb-9e69-212964e80092",
   "metadata": {},
   "source": [
    "### Homework 1: 尝试提出3个相关问题，测试 LCEL 构建的 RAG Chain 是否能成功召回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2d80e00-f35f-487d-a642-2c338841ef8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaze tracking has several important applications, particularly in robotics and human interaction interfaces, where understanding a person's object of interest is crucial. By accurately following a person's gaze, systems can predict future actions, as individuals often look at objects they intend to interact with before they actually do so. This capability enhances the interpretation of social interactions, allowing for better recognition of collaborative tasks among groups of people or understanding individual focus in varied contexts. Moreover, gaze tracking can improve action recognition in egocentric vision by identifying what a user is likely to engage with next. Overall, the ability to track gaze significantly contributes to the development of more intuitive and responsive technology that interacts seamlessly with human behavior."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What is the application on Gaze Tracking?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2492fae-652d-436f-b3ad-19c2bb9218ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gaze tracking model described utilizes a deep neural network architecture that combines head orientation and location information with scene content to predict where a person is looking within an image. Inputting a picture along with the person's head position, the model outputs a distribution over potential gaze targets, effectively functioning as a saliency map from the selected person's perspective. This approach is designed to operate in natural settings without restrictive assumptions, allowing it to handle various scenarios, such as multiple people interacting or looking at each other. The model is trained using the GazeFollow dataset, which captures diverse fixation scenarios and variations in gaze behavior. Overall, this model aims to emulate human gaze-following abilities, enhancing our understanding of social interactions in visual contexts."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"Descibe the model for Gaze Tracking?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36e86347-d7af-4f79-af04-af87e913bf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model demonstrates strong performance in predicting where people are looking, achieving an AUC of 0.878 and a mean Euclidean error of 0.190, significantly outperforming various baseline models. Notably, the model utilizes components such as head position and gaze pathways effectively, suggesting these features are crucial for accurate gaze estimation. However, it still remains below human performance, where a single annotator achieved an AUC of 0.924 and a mean error of 0.096. Qualitative results indicate that while the model can distinguish between different subjects and identify salient objects, it struggles with depth perception, leading to some inaccurate predictions. Overall, the model shows promise but highlights areas for improvement, particularly in achieving human-level accuracy."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"Evaluate the model.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464f370-bead-43b5-bcd3-711fc00e73a1",
   "metadata": {},
   "source": [
    "### Homework2: 测试对比两个提示词模板的召回率和生成质量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9e9565f-870e-494a-addd-256e7efcdd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zy/miniconda3/envs/llm-dev/lib/python3.12/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 使用 hub 模块拉取原本的 rag 提示词模板，并构建RAG chain\n",
    "prompt2 = hub.pull(\"rlm/rag-prompt\")\n",
    "# 使用 LCEL 构建 RAG Chain\n",
    "rag_chain2 = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e5855f5-bbc4-4616-99e5-273ddaba4dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaze tracking has several important applications, particularly in the fields of robotics and human interaction interfaces. By understanding where individuals are looking, systems can better interpret human intentions and actions, which is crucial for enhancing interaction quality. For instance, gaze-following can help predict what a person might do next, such as identifying objects they plan to interact with. Additionally, it can facilitate collaborative tasks by recognizing when multiple individuals are focusing on the same object. Overall, effective gaze tracking enhances the ability of computer vision systems to understand and interact with human behavior in natural settings."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain2.stream(\"What is the application on Gaze Tracking?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fb299dc-7783-4b31-86a0-3d124a886234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gaze-following model introduced in the context of GazeFollow leverages deep learning to predict where a person is looking within an image. It takes as input a picture and the location of a person's head, combining information about head orientation and location with scene content to output a distribution over potential gaze targets, effectively creating a saliency map from that person's perspective. This approach emulates human gaze-following behavior by first assessing the person's head and eyes to infer their line of sight, and then reasoning about salient objects within their view. The model is designed to handle various scenarios, including multiple people with joint attention or looking at each other, without relying on face detection or object-level annotations during training. The GazeFollow dataset serves as a benchmark for evaluating this model, providing a comprehensive resource for further research in gaze tracking."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain2.stream(\"Descibe the model for Gaze Tracking?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "289c43b9-b55a-4701-a717-ba0e818a0f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model demonstrates strong performance in predicting gaze direction, achieving an AUC of 0.878 and a mean Euclidean error of 0.190, which outperforms all baseline methods significantly. Notably, the model's results show a minimum L2 distance of 0.113 to the nearest ground truth fixation, indicating precise predictions. An ablation study reveals that all input components—image, position, and head—contribute positively to model performance, with the gaze pathway being particularly effective at estimating gaze direction. However, while the model shows promise, it still falls short of human performance, which has an AUC of 0.924 and a mean Euclidean error of 0.096, suggesting room for improvement. Overall, the model effectively distinguishes different people's points of view in images but faces challenges due to a lack of 3D understanding, leading to occasional inaccuracies."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain2.stream(\"Evaluate the model.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb05832c-02f9-4017-ba2e-03c52d63afbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
